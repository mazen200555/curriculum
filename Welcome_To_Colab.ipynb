{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mazen200555/curriculum/blob/master/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "rYYNvVSul1Rc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create positional encoding matrix\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        # Apply cosine to odd indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension and register as buffer (not a parameter)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encoding to input\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "MXo29Spcl7Lk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Single linear layer for all projections\n",
        "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
        "        self.c_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = 1 / math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_length, _ = x.size()\n",
        "\n",
        "        # Project inputs to queries, keys, and values\n",
        "        q, k, v = self.c_attn(x).chunk(3, dim=2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        # Apply causal (triangular) mask\n",
        "        if mask is None:\n",
        "            mask = torch.tril(torch.ones(seq_length, seq_length)).view(1, 1, seq_length, seq_length).to(x.device)\n",
        "\n",
        "        # Set masked positions to negative infinity\n",
        "        scores = scores.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.attn_dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back to original dimensions\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "        # Final projection\n",
        "        output = self.c_proj(context)\n",
        "        output = self.proj_dropout(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "w5H1Rd-al8FF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.c_fc = nn.Linear(d_model, d_ff)\n",
        "        self.c_proj = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first linear transformation and GELU activation\n",
        "        x = F.gelu(self.c_fc(x))\n",
        "        # Apply second linear transformation and dropout\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-TtpdcQ1l-2w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-12):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.weight * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "hlVSNSycmBKk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff=2048, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.ln_1 = LayerNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.ln_2 = LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection and layer normalization\n",
        "        x = x + self.attn(self.ln_1(x), mask)\n",
        "        # Feed-forward with residual connection and layer normalization\n",
        "        x = x + self.ff(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "bcomTRSZmC9S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=12, num_layers=12,\n",
        "                 max_seq_length=1024, dropout=0.1, d_ff=3072):\n",
        "        super(GPTModel, self).__init__()\n",
        "\n",
        "        # Token embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
        "        # Positional encoding\n",
        "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
        "\n",
        "        # Dropout for embeddings\n",
        "        self.emb_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Stack of transformer blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        # Final layer normalization\n",
        "        self.ln_f = LayerNorm(d_model)\n",
        "\n",
        "        # Output projection to vocabulary\n",
        "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # Store dimensions\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.max_seq_length\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.max_seq_length, f\"Cannot forward sequence of length {t}, max is {self.max_seq_length}\"\n",
        "\n",
        "        # Get token embeddings\n",
        "        token_embeddings = self.token_embedding(idx)  # (b, t, d_model)\n",
        "\n",
        "        # Get position embeddings\n",
        "        positions = torch.arange(0, t, dtype=torch.long, device=idx.device).unsqueeze(0)  # (1, t)\n",
        "        position_embeddings = self.position_embedding(positions)  # (1, t, d_model)\n",
        "\n",
        "        # Combine token and position embeddings\n",
        "        x = token_embeddings + position_embeddings  # (b, t, d_model)\n",
        "        x = self.emb_dropout(x)\n",
        "\n",
        "        # Create causal mask for self-attention\n",
        "        mask = torch.tril(torch.ones(t, t)).view(1, 1, t, t).to(idx.device)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Apply final layer normalization\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        logits = self.head(x)  # (b, t, vocab_size)\n",
        "\n",
        "        # Calculate loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Reshape logits and targets for loss calculation\n",
        "            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "\n",
        "        return logits, loss"
      ],
      "metadata": {
        "id": "w2RvglEYmE2o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptResponseDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=1024):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (str): Path to the CSV file with prompt-response pairs\n",
        "            tokenizer: Tokenizer to encode the text\n",
        "            max_length (int): Maximum sequence length\n",
        "        \"\"\"\n",
        "        # Load CSV data\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get prompt and response for the index\n",
        "        prompt = self.data.iloc[idx]['question']\n",
        "        response = self.data.iloc[idx]['answer']\n",
        "\n",
        "        # Combine prompt and response with separators\n",
        "        full_text = f\"{prompt}\\n{response}\"\n",
        "\n",
        "        # Tokenize the text\n",
        "        encodings = self.tokenizer(full_text,\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_length,\n",
        "                                  padding=\"max_length\",\n",
        "                                  return_tensors=\"pt\")\n",
        "\n",
        "        # Get input_ids and create target labels (shifted right)\n",
        "        input_ids = encodings['input_ids'].squeeze()\n",
        "\n",
        "        # For causal language modeling, targets are input_ids shifted right\n",
        "        targets = input_ids.clone()\n",
        "\n",
        "        # Shift targets right (first token predicts second token, etc.)\n",
        "        targets[:-1] = input_ids[1:]\n",
        "        # Last target is padding (will be masked by loss function)\n",
        "        targets[-1] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'targets': targets\n",
        "        }"
      ],
      "metadata": {
        "id": "9PkOVinumG8k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab_file=None):\n",
        "        \"\"\"\n",
        "        Simple tokenizer for illustration purposes.\n",
        "        In practice, you'd use a more sophisticated tokenizer like BPE.\n",
        "\n",
        "        Args:\n",
        "            vocab_file (str, optional): Path to vocabulary file\n",
        "        \"\"\"\n",
        "        if vocab_file:\n",
        "            with open(vocab_file, 'r', encoding='utf-8') as f:\n",
        "                self.vocab = {token.strip(): i for i, token in enumerate(f)}\n",
        "        else:\n",
        "            # Create a basic vocabulary with special tokens\n",
        "            self.vocab = {\n",
        "                \"<PAD>\": 0,\n",
        "                \"<UNK>\": 1,\n",
        "                \"<BOS>\": 2,\n",
        "                \"<EOS>\": 3,\n",
        "            }\n",
        "            # Add basic ASCII characters\n",
        "            for i in range(32, 127):\n",
        "                self.vocab[chr(i)] = len(self.vocab)\n",
        "\n",
        "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
        "\n",
        "    def __call__(self, text, truncation=False, max_length=None, padding=None, return_tensors=None):\n",
        "        \"\"\"Tokenize text to indices.\"\"\"\n",
        "        if isinstance(text, str):\n",
        "            tokens = list(text)  # Character-level tokenization for simplicity\n",
        "\n",
        "            # Convert tokens to ids\n",
        "            ids = [self.vocab.get(token, self.vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "            # Truncate if needed\n",
        "            if truncation and max_length is not None and len(ids) > max_length:\n",
        "                ids = ids[:max_length]\n",
        "\n",
        "            # Pad if needed\n",
        "            if padding == \"max_length\" and max_length is not None:\n",
        "                ids = ids + [self.vocab[\"<PAD>\"]] * (max_length - len(ids))\n",
        "\n",
        "            # Convert to tensor if requested\n",
        "            if return_tensors == \"pt\":\n",
        "                input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
        "            else:\n",
        "                input_ids = ids\n",
        "\n",
        "            return {\"input_ids\": input_ids}\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"Convert ids back to text.\"\"\"\n",
        "        if isinstance(ids, torch.Tensor):\n",
        "            ids = ids.tolist()\n",
        "\n",
        "        return \"\".join([self.id_to_token.get(id, \"<UNK>\") for id in ids])\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"Return the size of the vocabulary.\"\"\"\n",
        "        return len(self.vocab)"
      ],
      "metadata": {
        "id": "BQ0o3RTImJn3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataset, val_dataset=None, batch_size=4,\n",
        "                epochs=3, learning_rate=5e-5, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size) if val_dataset else None\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Calculate total training steps for scheduler\n",
        "    num_training_steps = len(train_loader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * num_training_steps),\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            # Get inputs and targets\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            targets = batch['targets'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, loss = model(input_ids, targets)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    targets = batch['targets'].to(device)\n",
        "                    _, loss = model(input_ids, targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "            model.train()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "W6jfa22umOXQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=50):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize prompt\n",
        "    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = prompt_tokens[\"input_ids\"].to(device)\n",
        "\n",
        "    # Track generated tokens\n",
        "    generated = input_ids.clone()\n",
        "\n",
        "    # Generate until max_length or until EOS token\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get predictions from model\n",
        "            logits, _ = model(generated)\n",
        "\n",
        "            # Focus on last token's prediction\n",
        "            next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k sampling\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = top_k_filtering(next_token_logits, top_k)\n",
        "                next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append new token to sequence\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token.item() == tokenizer.vocab.get(\"<EOS>\", None):\n",
        "                break\n",
        "\n",
        "    # Decode the generated text\n",
        "    output = tokenizer.decode(generated[0])\n",
        "\n",
        "    return output\n",
        "\n",
        "def top_k_filtering(logits, k):\n",
        "    \"\"\"Keep only the top k tokens with highest probability.\"\"\"\n",
        "    values, _ = torch.topk(logits, k)\n",
        "    min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])\n",
        "    return logits < min_values"
      ],
      "metadata": {
        "id": "MAZC68yXmRx8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = SimpleTokenizer()\n",
        "    vocab_size = tokenizer.get_vocab_size()\n",
        "\n",
        "    # Create model\n",
        "    model = GPTModel(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=256,         # Smaller for demonstration\n",
        "        num_heads=8,\n",
        "        num_layers=6,\n",
        "        max_seq_length=1024,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    # Prepare dataset\n",
        "    train_dataset = PromptResponseDataset(\"/content/Conversation.csv\", tokenizer)\n",
        "    val_dataset = PromptResponseDataset(\"/content/Conversation.csv\", tokenizer)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        batch_size=4,\n",
        "        epochs=3,\n",
        "        learning_rate=5e-5\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), \"gpt_model.pth\")\n",
        "\n",
        "    # Generate text with the trained model\n",
        "    prompt = \"What is machine learning?\"\n",
        "    generated_text = generate_text(model, tokenizer, prompt, max_length=100)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "z6xO5BBEmSgq",
        "outputId": "214c7de1-ec76-425f-e155-43f8cb3ee0ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Train Loss: 0.3823\n",
            "Epoch 1/3, Validation Loss: 0.1469\n",
            "Epoch 2/3, Train Loss: 0.1448\n",
            "Epoch 2/3, Validation Loss: 0.1371\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}