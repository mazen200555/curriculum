{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mazen200555/curriculum/blob/master/Small_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Configuration optimized for 15GB VRAM\n",
        "class Config:\n",
        "    vocab_size = 50257  # GPT-2 vocabulary size\n",
        "    hidden_size = 768\n",
        "    num_hidden_layers = 10\n",
        "    num_attention_heads = 12\n",
        "    intermediate_size = 3072\n",
        "    hidden_dropout_prob = 0.1\n",
        "    attention_probs_dropout_prob = 0.1\n",
        "    max_position_embeddings = 1024\n",
        "    layer_norm_eps = 1e-12\n",
        "    gradient_accumulation_steps = 8\n",
        "    batch_size = 1\n",
        "    learning_rate = 5e-5\n",
        "    epochs = 3\n",
        "    warmup_steps = 1000\n",
        "    max_grad_norm = 1.0\n",
        "    use_mixed_precision = True\n",
        "\n",
        "# Dataset class for text processing\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=1024):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "\n",
        "        for text in texts:\n",
        "            encodings_dict = tokenizer(text, truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attention_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_masks[idx],\n",
        "            'labels': self.input_ids[idx].clone()  # For autoregressive language modeling\n",
        "        }\n",
        "\n",
        "# 1. Dynamic Attention Mechanism\n",
        "class DynamicAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_attention_heads, dropout_prob=0.1):\n",
        "        super(DynamicAttention, self).__init__()\n",
        "\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = hidden_size // num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        # Regular attention components\n",
        "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
        "\n",
        "        # Complexity estimator determines optimal attention mode\n",
        "        self.complexity_estimator = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 4, 3)  # 3 modes: local, global, specialized\n",
        "        )\n",
        "\n",
        "        # Different attention modes for different input types\n",
        "        self.local_projector = nn.Linear(hidden_size, hidden_size)\n",
        "        self.global_projector = nn.Linear(hidden_size, hidden_size)\n",
        "        self.specialized_projector = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.output_linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        batch_size, seq_length, _ = hidden_states.size()\n",
        "\n",
        "        # Determine complexity and attention mode\n",
        "        avg_token_repr = hidden_states.mean(dim=1)\n",
        "        complexity_scores = F.softmax(self.complexity_estimator(avg_token_repr), dim=-1)\n",
        "\n",
        "        # Compute regular attention components\n",
        "        mixed_query = self.query(hidden_states)\n",
        "        mixed_key = self.key(hidden_states)\n",
        "        mixed_value = self.value(hidden_states)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        query = mixed_query.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        key = mixed_key.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        value = mixed_value.view(batch_size, seq_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attention_mask = (1.0 - attention_mask) * -10000.0\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Apply softmax and dropout\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Compute context vectors\n",
        "        context = torch.matmul(attention_probs, value)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.all_head_size)\n",
        "\n",
        "        # Apply different attention modes based on complexity\n",
        "        local_context = self.local_projector(context) * complexity_scores[:, 0].unsqueeze(1).unsqueeze(2)\n",
        "        global_context = self.global_projector(context) * complexity_scores[:, 1].unsqueeze(1).unsqueeze(2)\n",
        "        specialized_context = self.specialized_projector(context) * complexity_scores[:, 2].unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Combine different attention modes\n",
        "        output = local_context + global_context + specialized_context\n",
        "        output = self.output_linear(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 2. Multi-Resolution Token Processing\n",
        "class MultiResolutionTokenProcessor(nn.Module):\n",
        "    def __init__(self, hidden_size, num_resolutions=3):\n",
        "        super(MultiResolutionTokenProcessor, self).__init__()\n",
        "        self.num_resolutions = num_resolutions\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Different \"resolution\" processors\n",
        "        self.processors = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size, hidden_size),\n",
        "                nn.LayerNorm(hidden_size),\n",
        "                nn.GELU()\n",
        "            ) for _ in range(num_resolutions)\n",
        "        ])\n",
        "\n",
        "        # Resolution pooling/unpooling layers\n",
        "        self.pooling_layers = nn.ModuleList([\n",
        "            nn.Conv1d(hidden_size, hidden_size, kernel_size=2**i, stride=2**i, padding=0)\n",
        "            for i in range(1, num_resolutions)\n",
        "        ])\n",
        "\n",
        "        self.unpooling_layers = nn.ModuleList([\n",
        "            nn.ConvTranspose1d(hidden_size, hidden_size, kernel_size=2**i, stride=2**i, padding=0)\n",
        "            for i in range(1, num_resolutions)\n",
        "        ])\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_length, hidden_size = hidden_states.size()\n",
        "\n",
        "        # Process at original resolution\n",
        "        base_output = self.processors[0](hidden_states)\n",
        "        outputs = [base_output]\n",
        "\n",
        "        # Process at lower resolutions\n",
        "        current_hidden = hidden_states.transpose(1, 2)  # [batch, hidden, seq_len]\n",
        "\n",
        "        for i in range(1, self.num_resolutions):\n",
        "            # Pool to lower resolution\n",
        "            if seq_length >= 2**i:  # Only pool if sequence is long enough\n",
        "                pooled = self.pooling_layers[i-1](current_hidden)\n",
        "\n",
        "                # Process at this resolution\n",
        "                pooled = pooled.transpose(1, 2)  # [batch, reduced_seq, hidden]\n",
        "                processed = self.processors[i](pooled)\n",
        "\n",
        "                # Unpool back to original resolution\n",
        "                processed = processed.transpose(1, 2)  # [batch, hidden, reduced_seq]\n",
        "                # Ensure the unpooling can restore to the right size\n",
        "                padding_needed = seq_length - processed.size(2) * (2**i)\n",
        "                if padding_needed > 0:\n",
        "                    processed = F.pad(processed, (0, padding_needed))\n",
        "\n",
        "                upsampled = self.unpooling_layers[i-1](processed)\n",
        "\n",
        "                # Trim to original sequence length if needed\n",
        "                if upsampled.size(2) > seq_length:\n",
        "                    upsampled = upsampled[:, :, :seq_length]\n",
        "\n",
        "                # Add this resolution's contribution\n",
        "                outputs.append(upsampled.transpose(1, 2))  # [batch, seq, hidden]\n",
        "\n",
        "        # Combine outputs from all resolutions\n",
        "        combined_output = torch.stack(outputs, dim=0).sum(dim=0)\n",
        "        return combined_output\n",
        "\n",
        "# 3. Sparse-Dense Hybrid Layer\n",
        "class SparseDenseHybridLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size, sparsity_ratio=0.8):\n",
        "        super(SparseDenseHybridLayer, self).__init__()\n",
        "        self.sparsity_ratio = sparsity_ratio\n",
        "\n",
        "        # Dense processing path\n",
        "        self.dense_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_size, intermediate_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(intermediate_size, hidden_size)\n",
        "        )\n",
        "\n",
        "        # Sparse processing path (using grouped convolutions)\n",
        "        self.sparse_groups = 8\n",
        "        self.sparse_ff = nn.Sequential(\n",
        "            nn.Conv1d(hidden_size, intermediate_size, kernel_size=1, groups=self.sparse_groups),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(intermediate_size, hidden_size, kernel_size=1, groups=self.sparse_groups)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # Dense processing path\n",
        "        dense_output = self.dense_ff(hidden_states)\n",
        "\n",
        "        # Sparse processing path (using grouped convolutions)\n",
        "        batch_size, seq_length, hidden_size = hidden_states.size()\n",
        "        sparse_input = hidden_states.transpose(1, 2)  # [batch, hidden, seq]\n",
        "        sparse_output = self.sparse_ff(sparse_input)\n",
        "        sparse_output = sparse_output.transpose(1, 2)  # [batch, seq, hidden]\n",
        "\n",
        "        # Combine with sparsity ratio\n",
        "        output = (1 - self.sparsity_ratio) * dense_output + self.sparsity_ratio * sparse_output\n",
        "        return output\n",
        "\n",
        "# 4. Adaptive Parameter Efficiency\n",
        "class AdaptiveParameterModule(nn.Module):\n",
        "    def __init__(self, hidden_size, num_experts=4, expert_size=None):\n",
        "        super(AdaptiveParameterModule, self).__init__()\n",
        "        if expert_size is None:\n",
        "            expert_size = hidden_size * 2\n",
        "\n",
        "        # Create multiple expert networks\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size, expert_size),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(expert_size, hidden_size)\n",
        "            ) for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # Router network decides which experts to use for each input\n",
        "        self.router = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, num_experts)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # Get routing probabilities for each token\n",
        "        batch_size, seq_length, hidden_size = hidden_states.size()\n",
        "\n",
        "        # Average the token representations for routing\n",
        "        avg_repr = hidden_states.mean(dim=1)  # [batch, hidden]\n",
        "\n",
        "        # Compute routing weights\n",
        "        routing_weights = F.softmax(self.router(avg_repr), dim=-1)  # [batch, num_experts]\n",
        "\n",
        "        # Apply each expert\n",
        "        expert_outputs = []\n",
        "        for i, expert in enumerate(self.experts):\n",
        "            expert_output = expert(hidden_states)\n",
        "            # Weight this expert's output by its routing probability\n",
        "            weighted_output = expert_output * routing_weights[:, i].unsqueeze(1).unsqueeze(2)\n",
        "            expert_outputs.append(weighted_output)\n",
        "\n",
        "        # Combine all expert outputs\n",
        "        combined_output = torch.stack(expert_outputs).sum(dim=0)\n",
        "        return combined_output\n",
        "\n",
        "# 5. Advanced Transformer Layer\n",
        "class AdvancedTransformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AdvancedTransformerLayer, self).__init__()\n",
        "        self.attention = DynamicAttention(\n",
        "            config.hidden_size,\n",
        "            config.num_attention_heads,\n",
        "            config.attention_probs_dropout_prob\n",
        "        )\n",
        "        self.multi_resolution = MultiResolutionTokenProcessor(config.hidden_size)\n",
        "        self.sparse_dense = SparseDenseHybridLayer(config.hidden_size, config.intermediate_size)\n",
        "        self.adaptive_params = AdaptiveParameterModule(config.hidden_size)\n",
        "\n",
        "        self.attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.output_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Self attention with residual connection\n",
        "        attention_output = self.attention(self.attention_layernorm(hidden_states), attention_mask)\n",
        "        hidden_states = hidden_states + attention_output\n",
        "\n",
        "        # Process tokens at multiple resolutions\n",
        "        multi_res_output = self.multi_resolution(hidden_states)\n",
        "        hidden_states = hidden_states + multi_res_output\n",
        "\n",
        "        # Apply sparse-dense processing\n",
        "        intermediate_output = self.sparse_dense(self.output_layernorm(hidden_states))\n",
        "        hidden_states = hidden_states + intermediate_output\n",
        "\n",
        "        # Add adaptive parameter computation\n",
        "        adaptive_output = self.adaptive_params(hidden_states)\n",
        "        hidden_states = hidden_states + adaptive_output\n",
        "\n",
        "        return hidden_states, None\n",
        "\n",
        "# 6. Main Creative Language Model\n",
        "class CreativeLanguageModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CreativeLanguageModel, self).__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Token embeddings\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "\n",
        "        # Position embeddings\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "        # Layer stack with our advanced transformer layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            AdvancedTransformerLayer(config) for _ in range(config.num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "        # Output normalization and projection\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.output_projection = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "        # Tie weights between input embeddings and output projection\n",
        "        self.output_projection.weight = self.token_embeddings.weight\n",
        "\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        batch_size, seq_length = input_ids.size()\n",
        "\n",
        "        # Create position IDs\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Get embeddings\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        # Combine embeddings\n",
        "        hidden_states = token_embeddings + position_embeddings\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.layers:\n",
        "            hidden_states, _ = layer(hidden_states, attention_mask)\n",
        "\n",
        "        # Final layer norm and output projection\n",
        "        hidden_states = self.layer_norm(hidden_states)\n",
        "        logits = self.output_projection(hidden_states)\n",
        "\n",
        "        # Calculate loss if labels provided\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        return {'logits': logits, 'loss': loss}\n",
        "\n",
        "# 7. Data loading and preparation\n",
        "def load_and_prepare_data(config):\n",
        "    # Use the Hugging Face datasets library to load a small dataset\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
        "\n",
        "    # Convert dataset to list of texts (just take a portion for faster training)\n",
        "    train_texts = dataset[\"train\"][\"text\"][:5000]\n",
        "    val_texts = dataset[\"validation\"][\"text\"][:500]\n",
        "\n",
        "    # Remove empty strings\n",
        "    train_texts = [text for text in train_texts if text.strip()]\n",
        "    val_texts = [text for text in val_texts if text.strip()]\n",
        "\n",
        "    # Use the GPT-2 tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TextDataset(train_texts, tokenizer, max_length=config.max_position_embeddings)\n",
        "    val_dataset = TextDataset(val_texts, tokenizer, max_length=config.max_position_embeddings)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, tokenizer\n",
        "\n",
        "# 8. Training loop with memory optimization\n",
        "def train_model(model, train_loader, val_loader, config):\n",
        "    # Set up optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=config.learning_rate,\n",
        "        steps_per_epoch=len(train_loader) // config.gradient_accumulation_steps,\n",
        "        epochs=config.epochs\n",
        "    )\n",
        "\n",
        "    # Mixed precision training for memory efficiency\n",
        "    scaler = GradScaler() if config.use_mixed_precision else None\n",
        "\n",
        "    # Move model to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Track best validation loss\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs} [Train]\")\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, batch in enumerate(progress_bar):\n",
        "            # Get the inputs\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with autocast() if config.use_mixed_precision else nullcontext():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs['loss']\n",
        "                loss = loss / config.gradient_accumulation_steps\n",
        "\n",
        "            # Backward pass with mixed precision\n",
        "            if config.use_mixed_precision:\n",
        "                scaler.scale(loss).backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if (i + 1) % config.gradient_accumulation_steps == 0:\n",
        "                # Clip gradients\n",
        "                if config.use_mixed_precision:\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n",
        "\n",
        "                # Update weights\n",
        "                if config.use_mixed_precision:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_loss += loss.item() * config.gradient_accumulation_steps\n",
        "            progress_bar.set_postfix({'loss': train_loss / (i + 1)})\n",
        "\n",
        "            # Explicitly delete variables and garbage collect\n",
        "            del input_ids, attention_mask, labels, outputs, loss\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config.epochs} [Val]\")\n",
        "\n",
        "            for i, batch in enumerate(progress_bar):\n",
        "                # Get the inputs\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs['loss']\n",
        "\n",
        "                # Update validation loss\n",
        "                val_loss += loss.item()\n",
        "                progress_bar.set_postfix({'loss': val_loss / (i + 1)})\n",
        "\n",
        "                # Explicitly delete variables and garbage collect\n",
        "                del input_ids, attention_mask, labels, outputs, loss\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_val_loss,\n",
        "            }, 'best_creative_llm_model.pt')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.epochs} - Train Loss: {train_loss / len(train_loader):.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 9. Text generation function\n",
        "def generate_text(model, tokenizer, prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95):\n",
        "    \"\"\"Generate text given a prompt\"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Tokenize prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate tokens\n",
        "    generated_tokens = []\n",
        "    past_tokens = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Create attention mask\n",
        "            attention_mask = torch.ones_like(past_tokens)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(past_tokens, attention_mask=attention_mask)\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            # Get the next token logits from the last position\n",
        "            next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = torch.topk(next_token_logits, k=top_k)[0][:, -1].unsqueeze(-1) <= next_token_logits\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Convert logits to probabilities and sample\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Add to generated tokens\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Update past_tokens for next iteration\n",
        "            past_tokens = torch.cat((past_tokens, next_token), dim=1)\n",
        "\n",
        "    # Convert back to text\n",
        "    generated_ids = torch.cat([input_ids[0], torch.tensor(generated_tokens, device=device)], dim=0)\n",
        "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# 10. Main function to run everything\n",
        "def main():\n",
        "    # Set up configurations\n",
        "    config = Config()\n",
        "\n",
        "    # Apply memory optimizations\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Load and prepare data\n",
        "    train_loader, val_loader, tokenizer = load_and_prepare_data(config)\n",
        "\n",
        "    # Create model\n",
        "    model = CreativeLanguageModel(config)\n",
        "\n",
        "    # Print model summary\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model created with {total_params:,} total parameters\")\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(model, train_loader, val_loader, config)\n",
        "\n",
        "    # Generate sample text\n",
        "    prompts = [\n",
        "        \"In a world where AI has become\",\n",
        "        \"The solution to climate change is\",\n",
        "        \"Once upon a time in a distant galaxy\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\n=== Generated Text Samples ===\")\n",
        "    for prompt in prompts:\n",
        "        generated_text = generate_text(model, tokenizer, prompt, max_length=50)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Generated: {generated_text}\")\n",
        "\n",
        "    # Save the final model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'config': config\n",
        "    }, 'creative_llm_model.pt')\n",
        "\n",
        "# Define the nullcontext class for Python < 3.7\n",
        "class nullcontext:\n",
        "    def __init__(self, enter_result=None):\n",
        "        self.enter_result = enter_result\n",
        "    def __enter__(self):\n",
        "        return self.enter_result\n",
        "    def __exit__(self, *excinfo):\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "312qNCK81WLl",
        "outputId": "a5ecb41f-0119-4e63-d768-922d193cec7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created with 321,406,534 total parameters\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-9acb172101de>:418: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler() if config.use_mixed_precision else None\n",
            "Epoch 1/3 [Train]:   0%|          | 0/3227 [00:00<?, ?it/s]<ipython-input-11-9acb172101de>:442: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast() if config.use_mixed_precision else nullcontext():\n",
            "Epoch 1/3 [Train]: 100%|██████████| 3227/3227 [30:57<00:00,  1.74it/s, loss=1.01]\n",
            "Epoch 1/3 [Val]: 100%|██████████| 337/337 [02:15<00:00,  2.48it/s, loss=0.31]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3 - Train Loss: 1.0132 - Val Loss: 0.3104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 [Train]: 100%|██████████| 3227/3227 [30:31<00:00,  1.76it/s, loss=0.293]\n",
            "Epoch 2/3 [Val]: 100%|██████████| 337/337 [02:14<00:00,  2.51it/s, loss=0.196]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3 - Train Loss: 0.2929 - Val Loss: 0.1961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 [Train]:  99%|█████████▉| 3189/3227 [30:00<00:22,  1.72it/s, loss=0.207]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- extract_weights.py ---\n",
        "# PURPOSE: Load a PyTorch .pt model, extract weights/config, save as .npz/.json\n",
        "# REQUIREMENT: Must be run in an environment WITH PyTorch installed.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "# === IMPORTANT: Define Config and Model classes EXACTLY as in your training script ===\n",
        "# (Copy ALL the class definitions from your original training script here:\n",
        "# Config, DynamicAttention, MultiResolutionTokenProcessor, SparseDenseHybridLayer,\n",
        "# AdaptiveParameterModule, AdvancedTransformerLayer, CreativeLanguageModel)\n",
        "# --- Start of pasted classes ---\n",
        "\n",
        "# Configuration (Should match the one saved in your .pt file)\n",
        "class Config:\n",
        "    vocab_size = 50257\n",
        "    hidden_size = 768\n",
        "    num_hidden_layers = 10\n",
        "    num_attention_heads = 12\n",
        "    intermediate_size = 3072\n",
        "    hidden_dropout_prob = 0.1 # Not used in inference, but part of class def\n",
        "    attention_probs_dropout_prob = 0.1 # Not used in inference, but part of class def\n",
        "    max_position_embeddings = 1024\n",
        "    layer_norm_eps = 1e-12\n",
        "    # --- Add any other params your specific classes might need in __init__ ---\n",
        "    # Example defaults if they might be missing from a raw state dict:\n",
        "    sparsity_ratio = 0.8\n",
        "    num_experts = 4\n",
        "    sparse_groups = 8\n",
        "    num_resolutions = 3\n",
        "\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Paste ALL your model class definitions here ---\n",
        "# (DynamicAttention, MultiResolutionTokenProcessor, SparseDenseHybridLayer,\n",
        "#  AdaptiveParameterModule, AdvancedTransformerLayer, CreativeLanguageModel)\n",
        "# --- Make sure they are identical to the training script used ---\n",
        "# --- to generate creative_llm_model.pt                  ---\n",
        "\n",
        "# Example (YOU NEED TO PASTE THE FULL DEFINITIONS)\n",
        "class DynamicAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_attention_heads, dropout_prob=0.1):\n",
        "        super(DynamicAttention, self).__init__()\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = hidden_size // num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
        "        self.complexity_estimator = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 4, 3)\n",
        "        )\n",
        "        self.local_projector = nn.Linear(hidden_size, hidden_size)\n",
        "        self.global_projector = nn.Linear(hidden_size, hidden_size)\n",
        "        self.specialized_projector = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob) # Dropout ignored in eval()\n",
        "        self.output_linear = nn.Linear(hidden_size, hidden_size)\n",
        "    # --- forward method NOT needed for weight extraction ---\n",
        "\n",
        "class MultiResolutionTokenProcessor(nn.Module):\n",
        "    def __init__(self, hidden_size, num_resolutions=3):\n",
        "        super(MultiResolutionTokenProcessor, self).__init__()\n",
        "        self.num_resolutions = num_resolutions\n",
        "        self.hidden_size = hidden_size\n",
        "        self.processors = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size, hidden_size),\n",
        "                nn.LayerNorm(hidden_size),\n",
        "                nn.GELU()\n",
        "            ) for _ in range(num_resolutions)\n",
        "        ])\n",
        "        self.pooling_layers = nn.ModuleList([\n",
        "            nn.Conv1d(hidden_size, hidden_size, kernel_size=2**i, stride=2**i, padding=0)\n",
        "            for i in range(1, num_resolutions)\n",
        "        ])\n",
        "        self.unpooling_layers = nn.ModuleList([\n",
        "            nn.ConvTranspose1d(hidden_size, hidden_size, kernel_size=2**i, stride=2**i, padding=0)\n",
        "            for i in range(1, num_resolutions)\n",
        "        ])\n",
        "     # --- forward method NOT needed for weight extraction ---\n",
        "\n",
        "class SparseDenseHybridLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size, sparsity_ratio=0.8, sparse_groups=8): # Added sparse_groups default\n",
        "        super(SparseDenseHybridLayer, self).__init__()\n",
        "        self.sparsity_ratio = sparsity_ratio # This value might be needed in numpy impl\n",
        "        self.dense_ff = nn.Sequential(\n",
        "            nn.Linear(hidden_size, intermediate_size),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(intermediate_size, hidden_size)\n",
        "        )\n",
        "        self.sparse_groups = sparse_groups # Use parameter\n",
        "        self.sparse_ff = nn.Sequential(\n",
        "            nn.Conv1d(hidden_size, intermediate_size, kernel_size=1, groups=self.sparse_groups),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(intermediate_size, hidden_size, kernel_size=1, groups=self.sparse_groups)\n",
        "        )\n",
        "    # --- forward method NOT needed for weight extraction ---\n",
        "\n",
        "class AdaptiveParameterModule(nn.Module):\n",
        "    def __init__(self, hidden_size, num_experts=4, expert_size=None):\n",
        "        super(AdaptiveParameterModule, self).__init__()\n",
        "        if expert_size is None: expert_size = hidden_size * 2\n",
        "        self.num_experts = num_experts # Needed\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size, expert_size),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(expert_size, hidden_size)\n",
        "            ) for _ in range(num_experts)\n",
        "        ])\n",
        "        self.router = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, num_experts)\n",
        "        )\n",
        "    # --- forward method NOT needed for weight extraction ---\n",
        "\n",
        "class AdvancedTransformerLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(AdvancedTransformerLayer, self).__init__()\n",
        "        self.attention = DynamicAttention(\n",
        "            config.hidden_size, config.num_attention_heads, config.attention_probs_dropout_prob\n",
        "        )\n",
        "        # Pass necessary params from config\n",
        "        self.multi_resolution = MultiResolutionTokenProcessor(config.hidden_size, getattr(config, 'num_resolutions', 3))\n",
        "        self.sparse_dense = SparseDenseHybridLayer(config.hidden_size, config.intermediate_size, getattr(config, 'sparsity_ratio', 0.8), getattr(config, 'sparse_groups', 8))\n",
        "        self.adaptive_params = AdaptiveParameterModule(config.hidden_size, getattr(config, 'num_experts', 4))\n",
        "\n",
        "        self.attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.output_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "     # --- forward method NOT needed for weight extraction ---\n",
        "\n",
        "class CreativeLanguageModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CreativeLanguageModel, self).__init__()\n",
        "        self.config = config # Store config object\n",
        "        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            AdvancedTransformerLayer(config) for _ in range(config.num_hidden_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.output_projection = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        # Tie weights between input embeddings and output projection\n",
        "        self.output_projection.weight = self.token_embeddings.weight # Weight tying happens here\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob) # Ignored in eval()\n",
        "        # self.apply(self._init_weights) # Not needed for loading\n",
        "\n",
        "    # --- _init_weights and forward method NOT needed for weight extraction ---\n",
        "\n",
        "# --- End of pasted model classes ---\n",
        "\n",
        "# --- Configuration ---\n",
        "# *** USE THE PROVIDED PATH FOR THE INPUT MODEL ***\n",
        "MODEL_PATH = '/storage/emulated/0/Download/creative_llm_model.pt'\n",
        "\n",
        "# Output files will be saved in the current directory (relative paths)\n",
        "OUTPUT_NPZ_PATH = 'creative_llm_weights.npz'\n",
        "CONFIG_PATH = 'creative_llm_config.json'\n",
        "\n",
        "# --- Extraction Logic ---\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    # Provide a more specific error if the Android-like path isn't found\n",
        "    if MODEL_PATH.startswith('/storage/emulated/0/'):\n",
        "         print(f\"Error: Model file not found at '{MODEL_PATH}'.\")\n",
        "         print(\"Ensure the file exists in your device's 'Download' folder\")\n",
        "         print(\"and that this script has permission to read it.\")\n",
        "    else:\n",
        "         print(f\"Error: Model file not found: {MODEL_PATH}\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Loading model checkpoint from: {MODEL_PATH}\")\n",
        "# Handle CPU/GPU loading safely\n",
        "map_location = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using map_location: {map_location}\")\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(MODEL_PATH, map_location=map_location)\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- Error loading checkpoint with torch.load: {e} ---\")\n",
        "    print(\"This might happen due to:\")\n",
        "    print(\"  - Corrupted file.\")\n",
        "    print(\"  - Version mismatch between PyTorch used for saving and loading.\")\n",
        "    print(\"  - Insufficient RAM/permissions.\")\n",
        "    exit()\n",
        "\n",
        "# Determine if checkpoint is state_dict itself or a dictionary containing it\n",
        "if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    print(\"Loaded 'model_state_dict' from checkpoint dictionary.\")\n",
        "    # Try to load config from the same dictionary\n",
        "    config_data = checkpoint.get('config')\n",
        "    if config_data:\n",
        "         print(\"Loaded 'config' from checkpoint dictionary.\")\n",
        "    else:\n",
        "         print(\"Warning: 'config' not found in checkpoint dictionary. Using Config class defined above.\")\n",
        "         config_data = Config() # Fallback to class definition\n",
        "elif isinstance(checkpoint, OrderedDict):\n",
        "     model_state_dict = checkpoint\n",
        "     print(\"Loaded raw state_dict (checkpoint was likely the state_dict itself).\")\n",
        "     print(\"Warning: 'config' not found in raw state_dict. Using Config class defined above.\")\n",
        "     config_data = Config() # Fallback to class definition\n",
        "else:\n",
        "    print(\"\\n--- Error: Unexpected checkpoint format. ---\")\n",
        "    print(\"Expected a state_dict (OrderedDict) or a dict containing 'model_state_dict'.\")\n",
        "    print(f\"Got type: {type(checkpoint)}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Convert config object/class to a serializable dict\n",
        "if isinstance(config_data, Config): # If it's the class instance\n",
        "     config_dict = {k: v for k, v in config_data.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
        "elif isinstance(config_data, dict): # If it was already a dict\n",
        "     config_dict = config_data\n",
        "else:\n",
        "     print(\"Warning: Could not determine config format. Saving empty config.\")\n",
        "     config_dict = {}\n",
        "\n",
        "# Ensure core parameters are present in the final config_dict, using defaults if needed\n",
        "default_config = Config()\n",
        "for key, value in default_config.__dict__.items():\n",
        "    if not key.startswith('__') and not callable(value):\n",
        "        if key not in config_dict:\n",
        "            print(f\"Config Warning: Adding missing parameter '{key}' with default value '{value}'\")\n",
        "            config_dict[key] = value\n",
        "\n",
        "\n",
        "# Convert state_dict tensors to NumPy arrays\n",
        "numpy_weights = OrderedDict()\n",
        "print(\"\\nConverting weights to NumPy arrays...\")\n",
        "for key, tensor in model_state_dict.items():\n",
        "    print(f\"  Converting: {key} | Shape: {tensor.shape} | Dtype: {tensor.dtype}\")\n",
        "    # Ensure tensor is on CPU before converting to NumPy\n",
        "    numpy_weights[key] = tensor.cpu().numpy()\n",
        "\n",
        "print(f\"\\nSaving {len(numpy_weights)} weight arrays to: {OUTPUT_NPZ_PATH}\")\n",
        "np.savez(OUTPUT_NPZ_PATH, **numpy_weights)\n",
        "\n",
        "print(f\"Saving configuration to: {CONFIG_PATH}\")\n",
        "try:\n",
        "    with open(CONFIG_PATH, 'w') as f:\n",
        "        json.dump(config_dict, f, indent=4)\n",
        "except TypeError as e:\n",
        "    print(f\"\\n--- Error saving config to JSON: {e} ---\")\n",
        "    print(\"Some value in the config might not be JSON serializable.\")\n",
        "    print(\"Saving simplified config.\")\n",
        "    simplified_config = {k: str(v) for k, v in config_dict.items()} # Convert all to string as fallback\n",
        "    with open(CONFIG_PATH, 'w') as f:\n",
        "        json.dump(simplified_config, f, indent=4)\n",
        "\n",
        "\n",
        "print(\"\\n--- Weight Extraction Complete ---\")\n",
        "print(f\"NumPy weights saved to: '{OUTPUT_NPZ_PATH}'\")\n",
        "print(f\"Config saved to: '{CONFIG_PATH}'\")\n",
        "print(\"You can now use these files with the NumPy-based inference script.\")\n",
        "print(\"Ensure PyTorch is NOT required by the next script.\")"
      ],
      "metadata": {
        "id": "0GTgpsWFMudG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}